[0m[[0m[31merror[0m] [0m[0mjava.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor$.main(DataLakeProcessor.scala:56)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor.main(DataLakeProcessor.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:144)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:187)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1988)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1927)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:367)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:834)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor$.main(DataLakeProcessor.scala:14)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor.main(DataLakeProcessor.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:144)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:187)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1988)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1927)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:367)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:834)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:201)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor$.main(DataLakeProcessor.scala:14)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.example.DataLakeProcessor.main(DataLakeProcessor.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:144)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:94)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:187)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1988)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1927)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:367)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:834)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems[0m
